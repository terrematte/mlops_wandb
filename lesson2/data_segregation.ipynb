{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNwErBAQ+47prK7JSlGgTF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Dataset Splitting and Uploading to Weights & Biases (wandb)"],"metadata":{"id":"1NwXeevPMlnX"}},{"cell_type":"markdown","source":["In this exercise, the primary objective is to **split a pre-cleaned dataset** into **training** and **testing** subsets, and subsequently upload these subsets back to **``wandb``** as new artifacts. The dataset, housed in a **``wandb``** artifact named **``clean_data``**, comprises text files categorized into positive and negative sentiments, stored in two directories named **``pos``** and **``neg``**. The steps carried out in the script are detailed below:\n"],"metadata":{"id":"JKscKV8fRsJV"}},{"cell_type":"markdown","source":["\n","1. **Data Loading**:\n","\n","A function named **``load_data``** is defined to read the text files from the pos and neg directories, loading the text data into a list, and assigning labels of 1 for positive and 0 for negative sentiments.\n","\n","2. **Data Splitting**:\n","\n","A function named **``split_data``** is defined to split the loaded data into training and testing subsets. The **``train_test_split``** function from the sklearn library is utilized for this purpose, allowing for a configurable split ratio and random seed to ensure reproducibility.\n","\n","3. **Wandb Run Initialization**:\n","\n","A wandb run is initialized using **``wandb.init``** under the project **``my_user/sentiment_analysis``** with a job type of **``data_segregation``**. This initializes a new run on wandb to which the newly created artifacts will be logged.\n","\n","4. **Artifact Retrieval and Download**:\n","\n","The **``clean_data``** artifact is retrieved using run.use_artifact and its content is downloaded to the local directory using **``artifact.download``**.\n","\n","5. **Data Conversion and Saving**:\n","\n","The training and testing subsets are converted to Pandas DataFrames and saved to CSV files using the **``to_csv``** method. This creates local CSV files containing the text data and corresponding labels.\n","\n","6. **``Artifact Creation``**:\n","\n","Two new wandb artifacts named **``train_data``** and **``test_data``** are created to house the training and testing subsets, respectively. These artifacts are assigned types **``TrainData``** and **``TestData``**.\n","\n","7. **``File Addition to Artifacts``**:\n","\n","The local CSV files are added to the respective artifacts using the **``add_file``** method. This prepares the artifacts with the data for uploading to wandb.\n","\n","8. **``Artifact Uploading``**:\n","\n","The **``train_data``** and **``test_data``** artifacts are uploaded to wandb using **``run.log_artifact``**. This makes the split datasets available on wandb for further use.\n","\n","9. **``Wandb Run Termination``**:\n","\n","Optionally, the wandb run is terminated using wandb.finish to indicate the completion of the data splitting and uploading process.\n","\n","This script automates the process of splitting a dataset into training and testing subsets, and uploading these subsets to wandb as new artifacts. By doing so, the script facilitates the organized management and sharing of data, ensuring that the datasets are readily accessible for subsequent analysis or modeling tasks within the wandb environment."],"metadata":{"id":"lZ8VGjNjdY7s"}},{"cell_type":"markdown","source":["## Install, load libraries and setup wandb"],"metadata":{"id":"83gdylYTeogE"}},{"cell_type":"code","source":["!pip install wandb"],"metadata":{"id":"iVPo0SQDeuk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Login to Weights & Biases\n","!wandb login --relogin"],"metadata":{"id":"VlAKA94be51c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import wandb\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"pxIqzsevfBEd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Wandb Run Initialization, Artifact Retrieval and Download"],"metadata":{"id":"tDkJzE7hf4F_"}},{"cell_type":"code","source":["# Initialize wandb run\n","run = wandb.init(project='sentiment_analysis', job_type='data_segregation')\n","\n","# Get the clean_data artifact\n","artifact = run.use_artifact('clean_data:latest')\n","\n","# Download the content of the artifact to the local directory\n","data_path = artifact.download()"],"metadata":{"id":"uHreL48Df0Dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading and Split Data"],"metadata":{"id":"nJCcD7sEfZ3K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sbneJDAMboI"},"outputs":[],"source":["# Function to load data from the 'pos' and 'neg' directories\n","def load_data(data_path):\n","    data = []\n","    labels = []\n","    for sentiment in ['pos', 'neg']:\n","        sentiment_path = os.path.join(data_path, sentiment)\n","        for file_name in os.listdir(sentiment_path):\n","            with open(os.path.join(sentiment_path, file_name), 'r', encoding='utf-8') as file:\n","                data.append(file.read())\n","                labels.append(1 if sentiment == 'pos' else 0)\n","    return data, labels\n","\n","# Function to split data into training and testing sets\n","def split_data(data, labels, train_size=0.9, random_state=None):\n","    x_train, x_test, y_train, y_test = train_test_split(\n","        data, labels, train_size=train_size, random_state=random_state, stratify=labels)\n","    return x_train, x_test, y_train, y_test"]},{"cell_type":"markdown","source":["## All together"],"metadata":{"id":"qAqbQi1hgj74"}},{"cell_type":"code","source":["# Load data\n","data, labels = load_data(data_path)\n","\n","# Split data (default is 90% training, 10% testing, with a random state for reproducibility)\n","x_train, x_test, y_train, y_test = split_data(data, labels, random_state=42)"],"metadata":{"id":"ca2yX4KwfXC0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert split data to DataFrames\n","train_data = pd.DataFrame({'text': x_train, 'label': y_train})\n","test_data = pd.DataFrame({'text': x_test, 'label': y_test})\n","\n","# Log the shapes of the training and testing datasets\n","wandb.log({'train_data_shape': train_data.shape,\n","           'test_data_shape': test_data.shape})"],"metadata":{"id":"rnbjloMJgnBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.head()"],"metadata":{"id":"xRfUjh6hgyiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.shape"],"metadata":{"id":"DsE1kKAcg-U8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data.head()"],"metadata":{"id":"V8KKDa65g4gi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data.shape"],"metadata":{"id":"wMfRXORNhEuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save split data to CSV files\n","train_data.to_csv('train_data.csv', index=False)\n","test_data.to_csv('test_data.csv', index=False)\n","\n","# Create new artifacts for train and test data\n","train_artifact = wandb.Artifact(\n","    name='train_data',\n","    type='TrainData',\n","    description='Training data split from clean_data'\n",")\n","test_artifact = wandb.Artifact(\n","    name='test_data',\n","    type='TestData',\n","    description='Testing data split from clean_data'\n",")\n","\n","# Add CSV files to the artifacts\n","train_artifact.add_file('train_data.csv')\n","test_artifact.add_file('test_data.csv')\n","\n","# Log the new artifacts to wandb\n","run.log_artifact(train_artifact)\n","run.log_artifact(test_artifact)"],"metadata":{"id":"uipXUoZNguwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# finish the wandb run\n","# note that using run the files are update only after finish() is executed\n","wandb.finish()"],"metadata":{"id":"FG841VNgix5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"buZFUg_MjL2Z"},"execution_count":null,"outputs":[]}]}