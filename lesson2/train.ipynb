{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOmp1MMsDDu6KrJ59l1RadH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"050b80e812784b2f9a9cf092282bb02d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_6f6c5be300b64a229db98f9458bb7784","IPY_MODEL_6f38e078d99c4554b73f8af7a0e78e64"],"layout":"IPY_MODEL_709098d2892945118d4a035921bdcbca"}},"6f6c5be300b64a229db98f9458bb7784":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7444edb833a144758c35236535f7ce48","placeholder":"​","style":"IPY_MODEL_d83113a024b9487f87373ca502b5d2a1","value":"0.001 MB of 0.011 MB uploaded (0.000 MB deduped)\r"}},"6f38e078d99c4554b73f8af7a0e78e64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c27d2540d4774da08d3550dadba32276","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8959fcaa0475488ba7313cb5784562be","value":0.09265734265734266}},"709098d2892945118d4a035921bdcbca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7444edb833a144758c35236535f7ce48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d83113a024b9487f87373ca502b5d2a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c27d2540d4774da08d3550dadba32276":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8959fcaa0475488ba7313cb5784562be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Training a sentiment analysis model using artifacts from Wandb"],"metadata":{"id":"RbIAsG_ObtQk"}},{"cell_type":"markdown","source":["These steps collectively represent the setup, execution, and logging of a machine learning training session for sentiment analysis, fully integrated with the Wandb platform for experiment tracking and artifact management.\n","\n","1. **Wandb Initialization**:\n","   - The Wandb (Weights & Biases) session is initialized with the project name **``sentiment_analysis``** and the entity ``**your-user``**. This step associates the code execution with the specific project on Wandb for tracking experiments, logging metrics, and storing artifacts.\n","\n","2. **Downloading Artifacts**:\n","   - Two artifacts are used: one for training data (**``train_data:v0``**) and another for vocabulary (**``vocab:v0``**). The artifacts are specified by their names and versions, and they are downloaded using the **``artifact.download()``** method from Wandb. This method retrieves the artifacts from Wandb and saves them locally in the file system.\n","\n","3. **Loading Training Data**:\n","   - The **``load_train_data``** function is defined to read the downloaded **``train_data.csv``** using Pandas. This CSV file contains two columns: **``text``** for the reviews and **``label``** for the sentiments (0 for negative and 1 for positive). The function returns the text and labels as a Pandas Series and a NumPy array, respectively.\n","\n","4. **Loading Vocabulary**:\n","   - The **``load_vocab``** function opens the **``vocabulary.txt``** file from the downloaded vocabulary artifact directory. It reads all the lines, splits them into individual words, and then converts the list of words into a set. This set is used to filter the tokens in the training data so that only words present in the vocabulary are included.\n","\n","5. **Cleaning and Tokenizing Documents**:\n","   - The **``clean_doc``** function takes a document string, tokenizes it by whitespace, removes punctuation, filters out non-alphabetic tokens, stop words, and short tokens. This results in a list of clean tokens.\n","   - The **``filter_by_vocab``** function is applied to the list of training documents to retain only the tokens that are present in the vocabulary set.\n","\n","6. **Tokenization**:\n","   - A Keras **``Tokenizer``** is created using the **``create_tokenizer``** function which fits on the filtered training documents. The tokenizer converts the text documents into sequences of integers, where each integer represents a unique word in the vocabulary.\n","\n","7. **Data Encoding**:\n","   - The training documents are converted into a matrix representation with the **``texts_to_matrix``** method of the tokenizer object, using the **``freq``** mode to represent token frequency in the documents.\n","\n","8. **Model Definition**:\n","   - The **``define_model``** function constructs a Sequential neural network model with an input layer sized according to the number of words (features) and two Dense layers. The first Dense layer has 50 units with ReLU activation, and the second one is the output layer with a single unit and sigmoid activation, suitable for binary classification.\n","\n","9. **Model Training**:\n","   - The model is trained on the encoded training data for a predefined number of epochs using the **``fit``** method. During this process, the model learns to associate the input features with the sentiment labels.\n","\n","10. **Logging with Wandb**:\n","    - The training process logs the number of epochs, loss, and accuracy to Wandb using **``wandb.log``**. This allows for tracking the model's performance metrics in the Wandb dashboard.\n","\n","11. **Cleanup**:\n","    - Finally, the Wandb run is closed with **``wandb.finish()``** to signal that this run is complete, which helps in organizing and comparing runs within the Wandb interface."],"metadata":{"id":"4HwJcFORbw0C"}},{"cell_type":"markdown","source":["## Install, load libraries and setup wandb"],"metadata":{"id":"83gdylYTeogE"}},{"cell_type":"code","source":["!pip install wandb"],"metadata":{"id":"iVPo0SQDeuk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Login to Weights & Biases\n","!wandb login --relogin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlAKA94be51c","executionInfo":{"status":"ok","timestamp":1699271128533,"user_tz":180,"elapsed":8208,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"a577f6df-cd5e-4257-8098-3092e6864206"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["import string\n","import re\n","import pandas as pd\n","from numpy import array\n","from nltk.corpus import stopwords\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import wandb\n","import os\n","import nltk\n","from wandb.keras import WandbCallback"],"metadata":{"id":"ak9fOClhdG_a","executionInfo":{"status":"ok","timestamp":1699278232763,"user_tz":180,"elapsed":376,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Ensure that NLTK Stopwords are downloaded\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7aB8cuvCfE-o","executionInfo":{"status":"ok","timestamp":1699271710179,"user_tz":180,"elapsed":492,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"5ce401d5-ecff-4506-e610-dfce9500be2a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## Initialization, Wandb Run Setup and Artifact Download"],"metadata":{"id":"DSoyFlr1dnwz"}},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"id":"bQw4kJ9RZN69","executionInfo":{"status":"ok","timestamp":1699278298001,"user_tz":180,"elapsed":9075,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"b8d19a66-d22e-4578-bfeb-368fcea10718"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.12"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231106_134448-g8d2zqah</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/g8d2zqah' target=\"_blank\">rural-silence-14</a></strong> to <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/g8d2zqah' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/g8d2zqah</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n","\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"]}],"source":["# Initialize the W&B run\n","wandb.init(project=\"sentiment_analysis\", job_type=\"train\")\n","\n","# Use W&B artifact for training data\n","train_data_artifact = wandb.use_artifact('sentiment_analysis/train_data:v0', type='TrainData')\n","train_data_dir = train_data_artifact.download()\n","\n","# Use W&B artifact for validation data\n","test_data_artifact = wandb.use_artifact('sentiment_analysis/test_data:v0', type='TestData')\n","test_data_dir = test_data_artifact.download()\n","\n","# Use W&B artifact for vocabulary\n","vocab_artifact = wandb.use_artifact('sentiment_analysis/vocab:v0', type='Vocab')\n","vocab_dir = vocab_artifact.download()"]},{"cell_type":"markdown","source":["## Loading Vocabulary, Cleaning and Tokenizing Documents, Tokenization, Model Definition"],"metadata":{"id":"cV2ZuL9BvLkq"}},{"cell_type":"code","source":["# Function to load the training data\n","def load_data(data_dir):\n","    df = pd.read_csv(data_dir)\n","    return df['text'], array(df['label'])\n","\n","# Function to load the vocabulary\n","def load_vocab(vocab_dir):\n","    with open(vocab_dir, 'r') as file:\n","        vocab = file.read().split()\n","    return set(vocab)\n","\n","# Function to clean the documents\n","def clean_doc(doc):\n","    tokens = doc.split()\n","    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    tokens = [word for word in tokens if word.isalpha()]\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [w for w in tokens if not w in stop_words]\n","    tokens = [word for word in tokens if len(word) > 1]\n","    return tokens\n","\n","# Function to filter documents by vocabulary\n","def filter_by_vocab(docs, vocab):\n","    new_docs = []\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        tokens = [w for w in tokens if w in vocab]\n","        new_docs.append(' '.join(tokens))\n","    return new_docs\n","\n","# Function to create the tokenizer\n","def create_tokenizer(lines):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(lines)\n","    return tokenizer\n","\n","# Function to define the model\n","def define_model(n_words):\n","    model = Sequential()\n","    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","# Load the vocabulary\n","full_vocab_dir = os.path.join(vocab_dir, 'vocabulary.txt')\n","vocab = load_vocab(full_vocab_dir)\n","\n","# Load all reviews\n","# Train\n","full_train_data_dir = os.path.join(train_data_dir, 'train_data.csv')\n","train_docs, y_train = load_data(full_train_data_dir)\n","train_docs = filter_by_vocab(train_docs, vocab)\n","\n","## Create the tokenizer\n","tokenizer_train = create_tokenizer(train_docs)\n","\n","## Encode data\n","x_train = tokenizer_train.texts_to_matrix(train_docs, mode='freq')\n","\n","# Validation\n","full_test_data_dir = os.path.join(test_data_dir, 'test_data.csv')\n","test_docs, y_test = load_data(full_test_data_dir)\n","test_docs = filter_by_vocab(test_docs, vocab)\n","\n","## Encode data\n","x_test = tokenizer_train.texts_to_matrix(test_docs, mode='freq')\n","\n","# Define the model\n","n_words = x_train.shape[1]\n","model = define_model(n_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZR7UL6zdqt7","executionInfo":{"status":"ok","timestamp":1699278306103,"user_tz":180,"elapsed":3074,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"02ef6195-2fac-4f6b-800b-9cbb02530370"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_16 (Dense)            (None, 50)                1288550   \n","                                                                 \n"," dense_17 (Dense)            (None, 1)                 51        \n","                                                                 \n","=================================================================\n","Total params: 1288601 (4.92 MB)\n","Trainable params: 1288601 (4.92 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"FVPY31Hcvg3K"}},{"cell_type":"code","source":["# Fit network\n","model.fit(x_train,\n","          y_train,\n","          epochs=10,\n","          verbose=0,\n","          validation_data=(x_test,y_test),\n","          callbacks=[wandb.keras.WandbCallback(save_model=False,\n","                                                   compute_flops=True)])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3NZ5r4OvZ-_","executionInfo":{"status":"ok","timestamp":1699278331295,"user_tz":180,"elapsed":21801,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"2759c25b-fd9f-4264-c4dc-b247982bc004"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:453: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7c08e90eb190>"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# Finish the W&B run\n","wandb.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417,"referenced_widgets":["050b80e812784b2f9a9cf092282bb02d","6f6c5be300b64a229db98f9458bb7784","6f38e078d99c4554b73f8af7a0e78e64","709098d2892945118d4a035921bdcbca","7444edb833a144758c35236535f7ce48","d83113a024b9487f87373ca502b5d2a1","c27d2540d4774da08d3550dadba32276","8959fcaa0475488ba7313cb5784562be"]},"id":"fT-o8vZAet3H","executionInfo":{"status":"ok","timestamp":1699278339345,"user_tz":180,"elapsed":5175,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"f3602949-7b77-4393-b180-d2210b7a0bcc"},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050b80e812784b2f9a9cf092282bb02d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇▇█████</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>██▇▇▆▅▄▃▂▁</td></tr><tr><td>val_accuracy</td><td>▅▁▅▆█▇████</td></tr><tr><td>val_loss</td><td>██▇▆▆▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GFLOPs</td><td>0.00129</td></tr><tr><td>accuracy</td><td>0.96833</td></tr><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.50448</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.38895</td></tr><tr><td>val_accuracy</td><td>0.845</td></tr><tr><td>val_loss</td><td>0.50448</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">rural-silence-14</strong> at: <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/g8d2zqah' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/g8d2zqah</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231106_134448-g8d2zqah/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"f9C2wa_psW9z"},"execution_count":null,"outputs":[]}]}