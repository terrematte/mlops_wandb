{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and Logging Vocabulary as a Weights & Biases Artifact"
      ],
      "metadata": {
        "id": "gRcP6l-wmnjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Python script performs the following tasks to create a **vocabulary** from the **``train_data.csv``** dataset and log the entire process to **``Weights & Biases (wandb)``**:\n",
        "\n",
        "1. **Initialization**:\n",
        "The script starts by downloading the Natural Language Toolkit (NLTK) stopwords to filter out common words that do not contribute significantly to the sentiment analysis.\n",
        "\n",
        "2. **Wandb Run Setup**:\n",
        "A new wandb run is initiated within the specified project and job type. This run will record all operations and log outputs related to generating the vocabulary.\n",
        "\n",
        "3. **Artifact Download**:\n",
        "The **``train_data.csv``** artifact, which contains the pre-cleaned training data, is fetched from wandb using **``run.use_artifact``** with the latest version tag. The artifact's content is then downloaded locally for processing.\n",
        "\n",
        "4. **Data Loading**:\n",
        "The training data is read into a Pandas DataFrame from the CSV file. The text data needed for the vocabulary is extracted into a list using the **``load_data_from_dataframe``** function.\n",
        "\n",
        "5. **Vocabulary Construction**:\n",
        "The **``add_docs_to_vocab``** function iterates over each document in the text data, cleans it, and updates the vocab Counter object with the resulting tokens.\n",
        "\n",
        "6. **Token Cleaning**:\n",
        "The **``clean_doc``** function processes each document to remove punctuation, non-alphabetic tokens, stopwords, and very short tokens to produce a list of meaningful tokens.\n",
        "\n",
        "7. **Vocabulary Logging**:\n",
        "The initial size of the vocabulary is logged to wandb, capturing the number of unique tokens before filtering.\n",
        "\n",
        "8. **Filtering Tokens**:\n",
        "Tokens with a minimum occurrence (defined as 2) are retained to ensure the vocabulary only contains words that appear more than once in the corpus.\n",
        "\n",
        "9. **Filtered Vocabulary Logging**:\n",
        "The size of the filtered vocabulary, i.e., the number of tokens that meet the minimum occurrence criterion, is also logged to wandb.\n",
        "\n",
        "10. **Vocabulary Saving**:\n",
        "The filtered list of tokens is saved to a local file vocabulary.csv, ready to be uploaded to wandb.\n",
        "\n",
        "11. **Artifact Creation and Uploading**:\n",
        "A new artifact named **``vocab``** of type **``Vocab``** is created, and the **``vocabulary.csv``** file is added to this artifact. The artifact is then logged to the wandb run, which uploads it to the wandb server.\n",
        "\n",
        "12. **Run Completion**:\n",
        "The wandb run is concluded using **``run.finish()``**, marking the end of the vocabulary generation and logging process.\n",
        "\n",
        "This script facilitates an automated, reproducible approach to generating a vocabulary from text data and ensures that the results are tracked and stored in a structured manner within the wandb platform. It showcases how to use wandb for artifact management, from data retrieval and preprocessing to artifact creation and logging."
      ],
      "metadata": {
        "id": "hGrqRZlWmtL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install, load libraries and setup wandb"
      ],
      "metadata": {
        "id": "83gdylYTeogE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "iVPo0SQDeuk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48ce35c-6d66-49fd-8ea3-6cc05556be27"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlAKA94be51c",
        "outputId": "87038c65-68a5-42b8-b3c1-bb9382d73a36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import os"
      ],
      "metadata": {
        "id": "G3vdhdFWn10F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that NLTK Stopwords are downloaded\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Lo-7LzoluG",
        "outputId": "f56ebd51-3e77-48f5-ce24-97038b026cbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization, Wandb Run Setup and Artifact Download"
      ],
      "metadata": {
        "id": "Wp1VXCTSouFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb run\n",
        "run = wandb.init(project='sentiment_analysis', job_type='generate_vocab')\n",
        "\n",
        "# Download the train_data.csv artifact\n",
        "artifact = run.use_artifact('train_data:latest')\n",
        "train_data_path = artifact.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "vYmCdfu3otmi",
        "outputId": "e404bfc8-a906-431c-91ce-59a4714f1862"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterrematte\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241127_185953-29fy1ocf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/terrematte/sentiment_analysis/runs/29fy1ocf' target=\"_blank\">fine-snowflake-13</a></strong> to <a href='https://wandb.ai/terrematte/sentiment_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/terrematte/sentiment_analysis' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/terrematte/sentiment_analysis/runs/29fy1ocf' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis/runs/29fy1ocf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading, Vocabulary Construction, Token Cleaning, Vocabulary Saving"
      ],
      "metadata": {
        "id": "NiAB9SoW5SpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load text data into memory from a Pandas DataFrame\n",
        "def load_data_from_dataframe(df):\n",
        "    return df['text'].tolist()\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# turn documents into clean tokens\n",
        "def add_docs_to_vocab(texts, vocab):\n",
        "    for doc in texts:\n",
        "        tokens = clean_doc(doc)\n",
        "        vocab.update(tokens)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "    # convert lines to a single blob of text\n",
        "    data = '\\n'.join(lines)\n",
        "    # open file\n",
        "    file = open(filename, 'w')\n",
        "    # write text\n",
        "    file.write(data)\n",
        "    # close file\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "DzW0a_avoZO4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All together"
      ],
      "metadata": {
        "id": "dtLM-DhK5lBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correctly specify the path to the train_data.csv file\n",
        "full_train_data_path = os.path.join(train_data_path, 'train_data.csv')\n",
        "\n",
        "# Load the training data\n",
        "train_data_df = pd.read_csv(full_train_data_path)\n",
        "\n",
        "# Load text data\n",
        "texts = load_data_from_dataframe(train_data_df)\n",
        "\n",
        "# Define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# Add all docs to vocab\n",
        "add_docs_to_vocab(texts, vocab)\n",
        "\n",
        "# Log the size of the vocab\n",
        "wandb.log({'initial_vocab_size': len(vocab)})\n",
        "\n",
        "# Keep tokens with a min occurrence\n",
        "min_occurrence = 2\n",
        "tokens = [k for k, c in vocab.items() if c >= min_occurrence]\n",
        "wandb.log({'filtered_vocab_size': len(tokens)})\n",
        "\n",
        "# Save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocabulary.txt')\n",
        "\n",
        "# Create a new artifact for the vocabulary CSV\n",
        "vocab_artifact = wandb.Artifact(\n",
        "    name='vocab',\n",
        "    type='Vocab',\n",
        "    description='Vocabulary from training data'\n",
        ")\n",
        "\n",
        "# Add CSV file to the artifact\n",
        "vocab_artifact.add_file('vocabulary.txt')\n",
        "\n",
        "# Log the new artifact to wandb\n",
        "run.log_artifact(vocab_artifact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sCYLNzt4kG8",
        "outputId": "42bec0db-35e0-4c7c-cd3d-c6487af69a9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Artifact vocab>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finish the wandb run and upload the artifacts to cloud\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "c6FrE5fx5ItO",
        "outputId": "064056af-1588-45ff-90a1-1ce00df41fca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>filtered_vocab_size</td><td>▁</td></tr><tr><td>initial_vocab_size</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>filtered_vocab_size</td><td>25637</td></tr><tr><td>initial_vocab_size</td><td>44014</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine-snowflake-13</strong> at: <a href='https://wandb.ai/terrematte/sentiment_analysis/runs/29fy1ocf' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis/runs/29fy1ocf</a><br/> View project at: <a href='https://wandb.ai/terrematte/sentiment_analysis' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241127_185953-29fy1ocf/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}