{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a sentiment analysis model using artifacts from Wandb"
      ],
      "metadata": {
        "id": "RbIAsG_ObtQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These steps collectively represent the setup, execution, and logging of a machine learning training session for sentiment analysis, fully integrated with the Wandb platform for experiment tracking and artifact management.\n",
        "\n",
        "1. **Wandb Initialization**:\n",
        "   - The Wandb (Weights & Biases) session is initialized with the project name **``sentiment_analysis``** and the entity ``**your-user``**. This step associates the code execution with the specific project on Wandb for tracking experiments, logging metrics, and storing artifacts.\n",
        "\n",
        "2. **Downloading Artifacts**:\n",
        "   - Two artifacts are used: one for training data (**``train_data:v0``**) and another for vocabulary (**``vocab:v0``**). The artifacts are specified by their names and versions, and they are downloaded using the **``artifact.download()``** method from Wandb. This method retrieves the artifacts from Wandb and saves them locally in the file system.\n",
        "\n",
        "3. **Loading Training Data**:\n",
        "   - The **``load_train_data``** function is defined to read the downloaded **``train_data.csv``** using Pandas. This CSV file contains two columns: **``text``** for the reviews and **``label``** for the sentiments (0 for negative and 1 for positive). The function returns the text and labels as a Pandas Series and a NumPy array, respectively.\n",
        "\n",
        "4. **Loading Vocabulary**:\n",
        "   - The **``load_vocab``** function opens the **``vocabulary.txt``** file from the downloaded vocabulary artifact directory. It reads all the lines, splits them into individual words, and then converts the list of words into a set. This set is used to filter the tokens in the training data so that only words present in the vocabulary are included.\n",
        "\n",
        "5. **Cleaning and Tokenizing Documents**:\n",
        "   - The **``clean_doc``** function takes a document string, tokenizes it by whitespace, removes punctuation, filters out non-alphabetic tokens, stop words, and short tokens. This results in a list of clean tokens.\n",
        "   - The **``filter_by_vocab``** function is applied to the list of training documents to retain only the tokens that are present in the vocabulary set.\n",
        "\n",
        "6. **Tokenization**:\n",
        "   - A Keras **``Tokenizer``** is created using the **``create_tokenizer``** function which fits on the filtered training documents. The tokenizer converts the text documents into sequences of integers, where each integer represents a unique word in the vocabulary.\n",
        "\n",
        "7. **Data Encoding**:\n",
        "   - The training documents are converted into a matrix representation with the **``texts_to_matrix``** method of the tokenizer object, using the **``freq``** mode to represent token frequency in the documents.\n",
        "\n",
        "8. **Model Definition**:\n",
        "   - The **``define_model``** function constructs a Sequential neural network model with an input layer sized according to the number of words (features) and two Dense layers. The first Dense layer has 50 units with ReLU activation, and the second one is the output layer with a single unit and sigmoid activation, suitable for binary classification.\n",
        "\n",
        "9. **Model Training**:\n",
        "   - The model is trained on the encoded training data for a predefined number of epochs using the **``fit``** method. During this process, the model learns to associate the input features with the sentiment labels.\n",
        "\n",
        "10. **Logging with Wandb**:\n",
        "    - The training process logs the number of epochs, loss, and accuracy to Wandb using **``wandb.log``**. This allows for tracking the model's performance metrics in the Wandb dashboard.\n",
        "\n",
        "11. **Cleanup**:\n",
        "    - Finally, the Wandb run is closed with **``wandb.finish()``** to signal that this run is complete, which helps in organizing and comparing runs within the Wandb interface."
      ],
      "metadata": {
        "id": "4HwJcFORbw0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install, load libraries and setup wandb"
      ],
      "metadata": {
        "id": "83gdylYTeogE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "iVPo0SQDeuk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fe9fa3-5676-4493-b6a8-8e567b2c469e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlAKA94be51c",
        "outputId": "0f1cd2f6-af52-4f03-e000-e1dcf0dbc0ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import wandb\n",
        "import os\n",
        "import nltk\n",
        "from wandb.integration.keras import WandbMetricsLogger"
      ],
      "metadata": {
        "id": "ak9fOClhdG_a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that NLTK Stopwords are downloaded\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aB8cuvCfE-o",
        "outputId": "0bd47089-767f-4ce7-d23b-0320b3606a08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization, Wandb Run Setup and Artifact Download"
      ],
      "metadata": {
        "id": "DSoyFlr1dnwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "bQw4kJ9RZN69",
        "outputId": "1d7220e7-80ea-4477-84a7-46d2e8c65440"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241127_190911-jk1lubck</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/terrematte/sentiment_analysis/runs/jk1lubck' target=\"_blank\">kind-flower-15</a></strong> to <a href='https://wandb.ai/terrematte/sentiment_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/terrematte/sentiment_analysis' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/terrematte/sentiment_analysis/runs/jk1lubck' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis/runs/jk1lubck</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        }
      ],
      "source": [
        "# Initialize the W&B run\n",
        "wandb.init(project=\"sentiment_analysis\", job_type=\"train\")\n",
        "\n",
        "# Use W&B artifact for training data\n",
        "train_data_artifact = wandb.use_artifact('sentiment_analysis/train_data:v0', type='TrainData')\n",
        "train_data_dir = train_data_artifact.download()\n",
        "\n",
        "# Use W&B artifact for validation data\n",
        "test_data_artifact = wandb.use_artifact('sentiment_analysis/test_data:v0', type='TestData')\n",
        "test_data_dir = test_data_artifact.download()\n",
        "\n",
        "# Use W&B artifact for vocabulary\n",
        "vocab_artifact = wandb.use_artifact('sentiment_analysis/vocab:v0', type='Vocab')\n",
        "vocab_dir = vocab_artifact.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Vocabulary, Cleaning and Tokenizing Documents, Tokenization, Model Definition"
      ],
      "metadata": {
        "id": "cV2ZuL9BvLkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the training data\n",
        "def load_data(data_dir):\n",
        "    df = pd.read_csv(data_dir)\n",
        "    return df['text'], array(df['label'])\n",
        "\n",
        "# Function to load the vocabulary\n",
        "def load_vocab(vocab_dir):\n",
        "    with open(vocab_dir, 'r') as file:\n",
        "        vocab = file.read().split()\n",
        "    return set(vocab)\n",
        "\n",
        "# Function to clean the documents\n",
        "def clean_doc(doc):\n",
        "    tokens = doc.split()\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# Function to filter documents by vocabulary\n",
        "def filter_by_vocab(docs, vocab):\n",
        "    new_docs = []\n",
        "    for doc in docs:\n",
        "        tokens = clean_doc(doc)\n",
        "        tokens = [w for w in tokens if w in vocab]\n",
        "        new_docs.append(' '.join(tokens))\n",
        "    return new_docs\n",
        "\n",
        "# Function to create the tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# Function to define the model\n",
        "def define_model(n_words):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Load the vocabulary\n",
        "full_vocab_dir = os.path.join(vocab_dir, 'vocabulary.txt')\n",
        "vocab = load_vocab(full_vocab_dir)\n",
        "\n",
        "# Load all reviews\n",
        "# Train\n",
        "full_train_data_dir = os.path.join(train_data_dir, 'train_data.csv')\n",
        "train_docs, y_train = load_data(full_train_data_dir)\n",
        "train_docs = filter_by_vocab(train_docs, vocab)\n",
        "\n",
        "## Create the tokenizer\n",
        "tokenizer_train = create_tokenizer(train_docs)\n",
        "\n",
        "## Encode data\n",
        "x_train = tokenizer_train.texts_to_matrix(train_docs, mode='freq')\n",
        "\n",
        "# Validation\n",
        "full_test_data_dir = os.path.join(test_data_dir, 'test_data.csv')\n",
        "test_docs, y_test = load_data(full_test_data_dir)\n",
        "test_docs = filter_by_vocab(test_docs, vocab)\n",
        "\n",
        "## Encode data\n",
        "x_test = tokenizer_train.texts_to_matrix(test_docs, mode='freq')\n",
        "\n",
        "# Define the model\n",
        "n_words = x_train.shape[1]\n",
        "model = define_model(n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "5ZR7UL6zdqt7",
        "outputId": "3b41808a-5d12-4e9f-a4fd-25b9550f4c18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │       \u001b[38;5;34m1,281,950\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281,950</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,282,001\u001b[0m (4.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,282,001</span> (4.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,282,001\u001b[0m (4.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,282,001</span> (4.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "FVPY31Hcvg3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit network\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10,\n",
        "          verbose=0,\n",
        "          validation_data=(x_test,y_test),\n",
        "          callbacks=[wandb.keras.WandbMetricsLogger(),\n",
        "                     wandb.keras.WandbModelCheckpoint(filepath='model.keras', save_best_only=True)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3NZ5r4OvZ-_",
        "outputId": "5c136154-2cb4-4e5c-d294-e32b55c3f41d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using `save_best_only`, ensure that the `filepath` argument contains formatting placeholders like `{epoch:02d}` or `{batch:02d}`. This ensures correct interpretation of the logged artifacts.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e745c341300>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finish the W&B run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "fT-o8vZAet3H",
        "outputId": "ee0ac7a7-f564-481b-dbcf-59e8143c61f9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▄▃▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>epoch/epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>███▇▇▆▆▅▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▂▃▁▄▆▄▄▅▅▇▇▇▇█▇▇█▇██</td></tr><tr><td>epoch/val_loss</td><td>██▇▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.995</td></tr><tr><td>epoch/epoch</td><td>9</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.12297</td></tr><tr><td>epoch/val_accuracy</td><td>0.86</td></tr><tr><td>epoch/val_loss</td><td>0.34533</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">kind-flower-15</strong> at: <a href='https://wandb.ai/terrematte/sentiment_analysis/runs/jk1lubck' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis/runs/jk1lubck</a><br/> View project at: <a href='https://wandb.ai/terrematte/sentiment_analysis' target=\"_blank\">https://wandb.ai/terrematte/sentiment_analysis</a><br/>Synced 5 W&B file(s), 0 media file(s), 21 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241127_190911-jk1lubck/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9C2wa_psW9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}