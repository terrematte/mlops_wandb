{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEXIQIYq6n3+3pqQ5rcUIe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Creating and Logging Vocabulary as a Weights & Biases Artifact"],"metadata":{"id":"gRcP6l-wmnjU"}},{"cell_type":"markdown","source":["The Python script performs the following tasks to create a **vocabulary** from the **``train_data.csv``** dataset and log the entire process to **``Weights & Biases (wandb)``**:\n","\n","1. **Initialization**:\n","The script starts by downloading the Natural Language Toolkit (NLTK) stopwords to filter out common words that do not contribute significantly to the sentiment analysis.\n","\n","2. **Wandb Run Setup**:\n","A new wandb run is initiated within the specified project and job type. This run will record all operations and log outputs related to generating the vocabulary.\n","\n","3. **Artifact Download**:\n","The **``train_data.csv``** artifact, which contains the pre-cleaned training data, is fetched from wandb using **``run.use_artifact``** with the latest version tag. The artifact's content is then downloaded locally for processing.\n","\n","4. **Data Loading**:\n","The training data is read into a Pandas DataFrame from the CSV file. The text data needed for the vocabulary is extracted into a list using the **``load_data_from_dataframe``** function.\n","\n","5. **Vocabulary Construction**:\n","The **``add_docs_to_vocab``** function iterates over each document in the text data, cleans it, and updates the vocab Counter object with the resulting tokens.\n","\n","6. **Token Cleaning**:\n","The **``clean_doc``** function processes each document to remove punctuation, non-alphabetic tokens, stopwords, and very short tokens to produce a list of meaningful tokens.\n","\n","7. **Vocabulary Logging**:\n","The initial size of the vocabulary is logged to wandb, capturing the number of unique tokens before filtering.\n","\n","8. **Filtering Tokens**:\n","Tokens with a minimum occurrence (defined as 2) are retained to ensure the vocabulary only contains words that appear more than once in the corpus.\n","\n","9. **Filtered Vocabulary Logging**:\n","The size of the filtered vocabulary, i.e., the number of tokens that meet the minimum occurrence criterion, is also logged to wandb.\n","\n","10. **Vocabulary Saving**:\n","The filtered list of tokens is saved to a local file vocabulary.csv, ready to be uploaded to wandb.\n","\n","11. **Artifact Creation and Uploading**:\n","A new artifact named **``vocab``** of type **``Vocab``** is created, and the **``vocabulary.csv``** file is added to this artifact. The artifact is then logged to the wandb run, which uploads it to the wandb server.\n","\n","12. **Run Completion**:\n","The wandb run is concluded using **``run.finish()``**, marking the end of the vocabulary generation and logging process.\n","\n","This script facilitates an automated, reproducible approach to generating a vocabulary from text data and ensures that the results are tracked and stored in a structured manner within the wandb platform. It showcases how to use wandb for artifact management, from data retrieval and preprocessing to artifact creation and logging."],"metadata":{"id":"hGrqRZlWmtL2"}},{"cell_type":"markdown","source":["## Install, load libraries and setup wandb"],"metadata":{"id":"83gdylYTeogE"}},{"cell_type":"code","source":["!pip install wandb"],"metadata":{"id":"iVPo0SQDeuk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Login to Weights & Biases\n","!wandb login --relogin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlAKA94be51c","executionInfo":{"status":"ok","timestamp":1699257342113,"user_tz":180,"elapsed":30208,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"fe894264-b19d-4854-c4b3-208a053591a4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["import wandb\n","import pandas as pd\n","import string\n","import re\n","from collections import Counter\n","from nltk.corpus import stopwords\n","import nltk\n","import os"],"metadata":{"id":"G3vdhdFWn10F","executionInfo":{"status":"ok","timestamp":1699262067360,"user_tz":180,"elapsed":347,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Ensure that NLTK Stopwords are downloaded\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7Lo-7LzoluG","executionInfo":{"status":"ok","timestamp":1699257411389,"user_tz":180,"elapsed":577,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"2b08c7cd-9618-4759-983a-0a8a9128d18d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## Initialization, Wandb Run Setup and Artifact Download"],"metadata":{"id":"Wp1VXCTSouFx"}},{"cell_type":"code","source":["# Initialize wandb run\n","run = wandb.init(project='sentiment_analysis', job_type='generate_vocab')\n","\n","# Download the train_data.csv artifact\n","artifact = run.use_artifact('train_data:latest')\n","train_data_path = artifact.download()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"vYmCdfu3otmi","executionInfo":{"status":"ok","timestamp":1699257981262,"user_tz":180,"elapsed":5511,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"6d74bf71-727f-4f00-97fd-f93769ce91ce"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:i7w5oc30) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">dulcet-waterfall-8</strong> at: <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/i7w5oc30' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/i7w5oc30</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231106_080250-i7w5oc30/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:i7w5oc30). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.12"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231106_080615-51ddoscj</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/51ddoscj' target=\"_blank\">lively-mountain-9</a></strong> to <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/51ddoscj' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/51ddoscj</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"]}]},{"cell_type":"markdown","source":["## Data Loading, Vocabulary Construction, Token Cleaning, Vocabulary Saving"],"metadata":{"id":"NiAB9SoW5SpY"}},{"cell_type":"code","source":["# load text data into memory from a Pandas DataFrame\n","def load_data_from_dataframe(df):\n","    return df['text'].tolist()\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # remove remaining tokens that are not alphabetic\n","    tokens = [word for word in tokens if word.isalpha()]\n","    # filter out stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [w for w in tokens if not w in stop_words]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) > 1]\n","    return tokens\n","\n","# turn documents into clean tokens\n","def add_docs_to_vocab(texts, vocab):\n","    for doc in texts:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","\n","# save list to file\n","def save_list(lines, filename):\n","    # convert lines to a single blob of text\n","    data = '\\n'.join(lines)\n","    # open file\n","    file = open(filename, 'w')\n","    # write text\n","    file.write(data)\n","    # close file\n","    file.close()"],"metadata":{"id":"DzW0a_avoZO4","executionInfo":{"status":"ok","timestamp":1699261857014,"user_tz":180,"elapsed":488,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## All together"],"metadata":{"id":"dtLM-DhK5lBJ"}},{"cell_type":"code","source":["# Correctly specify the path to the train_data.csv file\n","full_train_data_path = os.path.join(train_data_path, 'train_data.csv')\n","\n","# Load the training data\n","train_data_df = pd.read_csv(full_train_data_path)\n","\n","# Load text data\n","texts = load_data_from_dataframe(train_data_df)\n","\n","# Define vocab\n","vocab = Counter()\n","\n","# Add all docs to vocab\n","add_docs_to_vocab(texts, vocab)\n","\n","# Log the size of the vocab\n","wandb.log({'initial_vocab_size': len(vocab)})\n","\n","# Keep tokens with a min occurrence\n","min_occurrence = 2\n","tokens = [k for k, c in vocab.items() if c >= min_occurrence]\n","wandb.log({'filtered_vocab_size': len(tokens)})\n","\n","# Save tokens to a vocabulary file\n","save_list(tokens, 'vocabulary.txt')\n","\n","# Create a new artifact for the vocabulary CSV\n","vocab_artifact = wandb.Artifact(\n","    name='vocab',\n","    type='Vocab',\n","    description='Vocabulary from training data'\n",")\n","\n","# Add CSV file to the artifact\n","vocab_artifact.add_file('vocabulary.txt')\n","\n","# Log the new artifact to wandb\n","run.log_artifact(vocab_artifact)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sCYLNzt4kG8","executionInfo":{"status":"ok","timestamp":1699262138337,"user_tz":180,"elapsed":1889,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"903ceec8-7d00-4c3f-f5b1-79c2fb737e1b"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Artifact vocab>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Finish the wandb run and upload the artifacts to cloud\n","run.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"c6FrE5fx5ItO","executionInfo":{"status":"ok","timestamp":1699262160987,"user_tz":180,"elapsed":5529,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"838941a7-cebf-4220-8cb4-8c9413ecbc56"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>filtered_vocab_size</td><td>▁</td></tr><tr><td>initial_vocab_size</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>filtered_vocab_size</td><td>25769</td></tr><tr><td>initial_vocab_size</td><td>44332</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">lively-mountain-9</strong> at: <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/51ddoscj' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/51ddoscj</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231106_080615-51ddoscj/logs</code>"]},"metadata":{}}]}]}