{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM/fn8McoYqEjin199+bOiC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Cleaning and Deduplication for Sentiment Analysis Dataset"],"metadata":{"id":"TbCmaSan8Mr5"}},{"cell_type":"markdown","source":["In this data cleaning exercise, the objective was to identify and remove duplicate text files from a sentiment analysis dataset hosted on **``Weights & Biases (wandb)``**, under the project **`my_user/sentiment_analysis`** with the artifact name **`txt_sentoken:v0`**. The dataset contains text files categorized into positive and negative sentiments, organized within two subfolders named **`pos`** and **`neg`**.\n","\n","Here are the steps undertaken to achieve the deduplication and re-upload the cleaned data back to wandb:\n","\n","1. **Wandb Initialization:**\n","   - A wandb run was initialized using **`wandb.init()`** under the project **`my_user/sentiment_analysis`** with the run name **`data_cleaning`**.\n","\n","2. **Artifact Retrieval:**\n","   - The **`txt_sentoken:v0`** artifact was retrieved from wandb using `wandb.use_artifact()` and its content was downloaded to the local directory using **`artifact.download()`**.\n","\n","3. **Hash Calculation:**\n","   - A function named **`calculate_hash`** was defined to compute the SHA-256 hash of a file, which serves as a unique identifier for the file content.\n","\n","4. **Duplicate Identification and Removal:**\n","   - A function named **`identify_and_remove_duplicates`** was defined to traverse through each file in the **`pos`** and **`neg`** subfolders, calculate the SHA-256 hash, and identify duplicate files. If a duplicate file was identified (based on the hash), it was removed from the directory using **`os.remove()`**.\n","\n","5. **Cleaned Data Artifact Creation:**\n","   - A new wandb artifact named **`clean_data`** was created to hold the cleaned dataset. This artifact was described as a dataset with duplicates removed.\n","\n","6. **Adding Cleaned Data to the Artifact:**\n","   - The cleaned data directories (**`pos`** and **`neg`** subfolders) were added to the **`clean_data`** artifact using **`clean_data_artifact.add_dir()`**.\n","\n","7. **Logging the Cleaned Data Artifact to Wandb:**\n","   - The **`clean_data`** artifact was logged to wandb using **`wandb.log_artifact()`**, making the cleaned dataset available for further analysis or modeling.\n","\n","8. **Wandb Run Termination (Optional):**\n","   - Optionally, the wandb run was concluded using **`wandb.finish()`** to indicate the end of the data cleaning run.\n","\n","Through these steps, a systematic approach was followed to download the original dataset, identify and remove duplicate files, and re-upload the cleaned dataset as a new artifact on wandb, ensuring a clean, deduplicated dataset ready for subsequent analysis or machine learning tasks."],"metadata":{"id":"Ph10GOMv8WX6"}},{"cell_type":"markdown","source":["## Install and load libraries"],"metadata":{"id":"uvyoasp29etb"}},{"cell_type":"code","source":["!pip install wandb"],"metadata":{"id":"JpyrV7Cs9m0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Login to Weights & Biases\n","!wandb login --relogin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zBDkbT59rom","executionInfo":{"status":"ok","timestamp":1699229409794,"user_tz":180,"elapsed":24168,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"150e5102-6e94-4855-8c26-86e0726e2561"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["import hashlib\n","import os\n","import wandb\n","import shutil"],"metadata":{"id":"AwZX0c4K9Zsl","executionInfo":{"status":"ok","timestamp":1699229415986,"user_tz":180,"elapsed":723,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Wandb Initialization and Artifact Retrieval"],"metadata":{"id":"LuUI0GVw-SD2"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"JPcisDSm7210","executionInfo":{"status":"ok","timestamp":1699229650449,"user_tz":180,"elapsed":28083,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"3107eb92-5d6b-4392-a614-75293175b027"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivanovitch-silva\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.12"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231106_001345-2fxlxivc</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/2fxlxivc' target=\"_blank\">dainty-wood-3</a></strong> to <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/2fxlxivc' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/2fxlxivc</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m:   2000 of 2000 files downloaded.  \n"]}],"source":["# Initialize wandb run\n","wandb.init(project='sentiment_analysis', job_type=\"preprocessing\")\n","\n","# Get the artifact\n","artifact = wandb.use_artifact('txt_sentoken:v0')\n","\n","# Download the content of the artifact to the local directory\n","artifact_dir = artifact.download()"]},{"cell_type":"markdown","source":["## Hash Calculation and Duplicate Identification and Removal"],"metadata":{"id":"VYMQ5PKQ-zZZ"}},{"cell_type":"code","source":["def calculate_hash(file_path):\n","    \"\"\"Calculate SHA-256 hash of a file.\"\"\"\n","    sha256_hash = hashlib.sha256()\n","    with open(file_path,\"rb\") as f:\n","        # Read and update hash in chunks of 4K\n","        for byte_block in iter(lambda: f.read(4096),b\"\"):\n","            sha256_hash.update(byte_block)\n","    return sha256_hash.hexdigest()\n","\n","def identify_and_remove_duplicates(folder_path):\n","    \"\"\"Identify duplicate files in a folder and remove them.\"\"\"\n","    file_hashes = {}\n","    for root, dirs, files in os.walk(folder_path):\n","        for file in files:\n","            file_path = os.path.join(root, file)\n","            file_hash = calculate_hash(file_path)\n","            if file_hash in file_hashes:\n","                log_message = f'Removing duplicate: {file_path}'\n","                wandb.log({'log': log_message})\n","                os.remove(file_path)\n","            else:\n","                file_hashes[file_hash] = file_path\n","\n","# Paths to 'pos' and 'neg' subfolders\n","pos_folder_path = os.path.join(artifact_dir, 'pos')\n","neg_folder_path = os.path.join(artifact_dir, 'neg')\n","\n","# Identify and remove duplicates in 'pos' and 'neg' subfolders\n","identify_and_remove_duplicates(pos_folder_path)\n","identify_and_remove_duplicates(neg_folder_path)"],"metadata":{"id":"ZFMjAMAb-N08","executionInfo":{"status":"ok","timestamp":1699229827953,"user_tz":180,"elapsed":346,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Cleaned Data Artifact Creation and Adding Cleaned Data to the Artifact"],"metadata":{"id":"rJSuWc7B_eXL"}},{"cell_type":"code","source":["# Create a new artifact for the clean data\n","clean_data_artifact = wandb.Artifact(\n","    name='clean_data',\n","    type='CleanData',\n","    description='Cleaned dataset with duplicates removed'\n",")\n","\n","# Add the cleaned data directories to the clean_data artifact\n","clean_data_artifact.add_dir(pos_folder_path, name='pos')\n","clean_data_artifact.add_dir(neg_folder_path, name='neg')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10nFgqQ7-6kq","executionInfo":{"status":"ok","timestamp":1699229969660,"user_tz":180,"elapsed":1203,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"395fdd6a-f2a8-4188-95f2-1696c2785300"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./artifacts/txt_sentoken:v0/pos)... Done. 0.6s\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./artifacts/txt_sentoken:v0/neg)... Done. 0.6s\n"]}]},{"cell_type":"markdown","source":["## Logging the Cleaned Data Artifact to Wandb and Terminate"],"metadata":{"id":"fkSEl4bEAAF9"}},{"cell_type":"code","source":["# Log the clean_data artifact to wandb\n","wandb.log_artifact(clean_data_artifact)\n","\n","# Optionally, finish the wandb run (if this is the end of your script)\n","wandb.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"Y1hyCdC0_4a2","executionInfo":{"status":"ok","timestamp":1699230064929,"user_tz":180,"elapsed":52233,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"c788344a-f8d0-4410-c75a-a9ac9db3b01c"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">dainty-wood-3</strong> at: <a href='https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/2fxlxivc' target=\"_blank\">https://wandb.ai/ivanovitch-silva/sentiment_analysis/runs/2fxlxivc</a><br/>Synced 5 W&B file(s), 0 media file(s), 2000 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231106_001345-2fxlxivc/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"tWFFHSY7AFIt"},"execution_count":null,"outputs":[]}]}